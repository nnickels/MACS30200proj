---
title: "A comparative digital survey investigation of the construct validity of the Trait Anxiety Inventory within a UChicago community sample and an MTurk sample"  
author: "Nora Nickels"
date: "4/23/2018"
header-includes:
    - \usepackage{setspace}
output: pdf_document
bibliography: references.bib
---

### Research Questions: 
* Do distributions of trait anxiety scores differ in samples acquired from a University of Chicago community vs. the Amazon Mechanial Turk community?
* How strong is the construct validity of the Trait Anxiety Inventory in a sample drawn from these two populations; specifically, do setting and mood relate to trait anxiety responses of UChicago or MTurk community members when the T.A.I. is completed outside of a controlled laboratory setting?

\doublespacing

# Literature Review

In psychological and biological research, emotions are measured both in terms of acute states of arousal and in terms of individual differences in the propensity to experience that emotion. Scientists define the difference between these two elements of emotional measurement as state and trait measures of emotions. For example, anxiety as an emotion can be generally defined as heightened feelings of tension, apprehension, and worry, in combination with an aroused physiological state [@spielberger2010state]. It is particularly important to distinguish between state and trait for anxiety, as higher trait anxiety, or higher individual proneness to experience anxiety, could affect the way an individual reacts behaviorally in both acute and long term situations. Trait anxiety can be defined as an individual measure of intensity and frequency of experienced anxiety, which involves these feelings of apprehension and heightened response of the autonomic nervous system [@spielberger1966theory]. Importantly, trait anxiety is seen as a relatively *stable* trait, and individuals who have higher trait anxiety tend to perceive situations as more dangerous or stressful over time [@spielberger1966theory].

The State-Trait Anxiety Inventory, or STAI, is a long-standing measure that uses two scales to report these two measures (state anxiety and trait anxiety)[@spielberger1989state]. The STAI is designed as a self-report measure, with items that map specifically onto the two factors of anxiety. The STAI trait scale consists of twenty statements that have individuals rate, on a four-point Likert scale, different statements about how they feel generally (e.g., "I feel nervous and restless."). Both the state and trait scales of the STAI are long-standing, frequently used scales in psychology, and theoretically, the inventory has been shown to measure response to experimental manipulation in meaningful ways [@chapman1977determinants]. Further, the two subscales have been shown to correlate with other measures of anxiety that is consistent the content of measure [@bieling1998state]. 

Importantly, retest correlations of the iventory have shown strong reliability, and re-test coefficients for the trait scale have shown to be even higher for those items that measure the trait scale [@spielberger2010state; @barnes2002reliability]. The STAI is reported to have high validity, with concurrent validity with other anxiety questionnaires reported as ranging from 0.73 - 0.85 [@bieling1998state]. However, some researchers argue that a general, yet incorrect, implication that is attached to re-test reliability is that which assumes that once an instrument is found to be reliable, its reliability does or cannot change [@barnes2002reliability]. If reliability is simply a property of scores from a specific sample of survey-takers, as opposed to being a property of the test itself, then reliability of a measure can be affected by any source of variability that also affects the scores (e.g., demographics in a particular sample, such as gender, age, motivation, mood, etc.)[@barnes2002reliability]. Therefore, although re-test reliability and concurrent measures of validity are incredibly important, considering the specific sample involved in one's study is crucial in discussion of the interpretation of one's results. 

In my work, I share equal concern in that my specific sample is taken from a community whose specific demographics may affect the distribution of anxiety scores. Like most psychology study populations, our work frequently involves participant samples drawn from a university setting. Specifically, the University of Chicago ranks as one of the top undergraduate research institutions in the U.S., and is often viewed as a competitive and stressful environment. Beyond the concern that many research institutions have about their willing research participants coming from a primarily Western, educated, industrialized, rich, and democractic (WEIRD) population [@jones2010weird], our lab also deals with the concern of recruiting willing participants from a sample that may not only have higher than usual scores of trait anxiety, but also have rapidly fluctuating rates of both state and trait anxiety throughout their academic experience.

To control some of these concerns of validity and reliability, researchers often use a controlled, laboratory setting, to remove extraneous effects of the environment. For example, our laboratory has research participants spend about twenty-five minutes in the laboratory before first saliva samples are taken, to reduce the potential for effects outside of the lab to result in hormonal concentrations that do not represent true baseline. In this way, we also administer many psychological surveys in the lab as well. Howevers, due to both time and monetary restraints, we occasionally adminster *trait* based questionnaires digitally in advance of the lab session, as trait based questionaires theoretically measure relatively stable personality measures. 

Digital surveys and digital ethnographic methods are seen as new technologies for social research that allow scientists to avoid more costly research methods, to easily alter questionnaires to access different cultural groups, to access hard-to-reach populations, to collect higher response rates, and to consolidate data more quickly and efficiently [@murthy2008digital]. In the case of administering our surveys digitally outside of the lab, we save both temporal and monetary costs, yet run the risk of extraneous factors of the environment interacting with demographics of our sample and therefore affecting the trait anxiety scores of our participants. If certain factors environmentally outside of a controlled laboratory could affect trait anxiety scores, then we experience a trade off of losing validity when our survey is administered digitally.

Obviously, our sample taken from the University of Chicago community is not the only sample from which digital survey data is drawn. Digitally web-based data collection is a relatively new method that contains the primary elements needed to conduct social scientific research, while benefitting from the same aspects discussed above. In fact, despite the concerns of losing the control of a laboratory environment, some researchers have argued that survey data that is digitally collected are in fact preferred to data that has been collected in-person. For example, Castler et al. compared data that had been collected in the lab and also online, and found that the test results themselves resulted in equivalent, high-quality data for both groups, and that the data collected digitally was in fact more socioeconomically and ethnically diverse [-@casler2013separate]. Further, Hauser and Schwarz found that data digitally collected using Amazon's Mechanical Turk (MTurk) showed higher rates of participant attentiveness (measured using attentiveness an instructional manipulation check) when compared to data collected from college students [-@hauser2016attentive]. 

On the other hand, other research that compares samples collected from digital populations with in-person samples have found differences that may be less beneficial. Further, even if both samples are collected digitally but come from differing populations, the samples themselves may compare and contrast in interesting ways, based on the population from which the digital survey sample is taken. For example, Goodman and colleagues compared MTurk participants with student samples on multiple measures, including attentiveness, personality, and certain decision-making biases [-@goodman2013data]. The authors found that MTurk participants were actually less attentive and had different personality profiles (e.g., less extroverted, less emotionally stable) when compared to a student population, but were similar in terms of how they value money and time and in terms of their risk aversion [@goodman2013data].  

Clearly then, the results of this line of research have been mixed thus far. What we can confirm is that much of the literature focusing on the strengths and weaknesses of digital data has focused specifically on globally digital as opposed to local populations, where data can be crowdsourced or collected in a completely digital way. Conducting psychology research using crowdsourced data has largely revolved around the Amazon Mechanical Turk (MTurk) platform, based on its popularity and ease of access.  MTurk provides a platform to outsource small tasks (referred to as HITS, or human intelligence tasks) to a workforce collected globally that is made up of "workers" [@behrend2011viability]. The MTurk platform has been investigated to confirm that it provides an efficient and reliable alternative from the university participant population [@behrend2011viability; @rand2012promise]. Further, MTurk has been used to successfully replicate experimental work, showing its viability in terms of experimental design and validity flexibility [@berinsky2012evaluating].  

In particular, a solid amount of work has been done investigating the specific MTurk population, focusing on the demographics, responsivity, and motivation of the community of MTurk workers. Many studies show that the demographics of MTurk workers fluctuate, and that depending on the research questions being asked, researchers must use caution when selecting participants by filtering targeted study pools on MTurk [@huff2015these; @ross2010crowdworkers; @casey2017intertemporal; ]. Others suggest that the pros and cons of using the MTurk pool are based on both controllable and uncontrollable factors, and that often the benefits, such as accessing hard-to-reach populations, exceed the downsides of the use of in-person populations and lab studies [@paolacci2014inside; @smith2015convenient]. Fields of psychology, political science, and industrial / organization psychology in particular pay particular attention to the personality characterisics and ideology of the MTurk pool, as those factors are incredibly important when considering the external validity of individual characteristics of one's research participants [@bates2013conducting; @clifford2015samples; @woo2015amazon]. Overall, there has been much discussion regarding the methodology of MTurk sampling and the MTurk population, as its promise of accessing high quality, inexpensive data is ground breaking to many lines of research [@buhrmester2011amazon].

Based on this literature and common restrictions of both money and time, our lab continues to have standing concerns based on the comparison between our sample population, drawn from the UChicago community and containing many undergraduate college students, and a sample population coming from a wider population, such as the the MTurk commmunity. Past research has discussed the pros and cons of data collection from in-person vs. digital methodologies, and it is critical to know the specific descriptive statistics of a specific sampling frame, and how these descriptions differ from other sample populations, such as a wider and argulably more externally valid, global community. In particular, our use of psychology research is invested in stable personality, emotional, and psychological traits that map on to biological and behavioral responses. Therefore, we are focused on the distribution of stable traits in our population, how this distribution differs from other samples, and how the scores that lead to this distribution are impacted by extraneous factors. This study seeks to answer, specifically, how the TAI scores of a sample from a UChicago community compare to this collected from a digitally crowdsourced sample from Amazon Mechanical Turk. These comparisons in scores could be tied to differences in specific anxiety traits between the two samples, or differences in diversity amongst the groups. Further, this study will look into the construct validity of both the UChicago sample and the MTurk sample, by focusing on how extraneous factors, such as setting and mood, affect the responses of the TAI for both a UChicago based sample and an MTurk collected sample. The focus on these factors will add to the literature surrounding how in-person vs. digitally collected data compare.


\pagebreak  
\singlespacing  

# References